{
  "accuracy": 0.5204435204435205,
  "f1_macro": 0.3615852189881316,
  "f1_weighted": 0.5155496239794224,
  "class_report": {
    "NEG": {
      "precision": 0.3753501400560224,
      "recall": 0.28210526315789475,
      "f1-score": 0.32211538461538464,
      "support": 475.0
    },
    "NEU": {
      "precision": 0.63125,
      "recall": 0.6824324324324325,
      "f1-score": 0.6558441558441559,
      "support": 888.0
    },
    "POS": {
      "precision": 0.0873015873015873,
      "recall": 0.1375,
      "f1-score": 0.10679611650485436,
      "support": 80.0
    },
    "accuracy": 0.5204435204435205,
    "macro avg": {
      "precision": 0.36463390911920324,
      "recall": 0.36734589853010907,
      "f1-score": 0.3615852189881316,
      "support": 1443.0
    },
    "weighted avg": {
      "precision": 0.5168575492104904,
      "recall": 0.5204435204435205,
      "f1-score": 0.5155496239794224,
      "support": 1443.0
    }
  },
  "confusion_matrix": [
    [
      134,
      306,
      35
    ],
    [
      202,
      606,
      80
    ],
    [
      21,
      48,
      11
    ]
  ],
  "experiment_config": {
    "preprocessing_pipeline": "remove_stopwords",
    "vectorization_method": "word2vec",
    "model_name": "logistic_regression",
    "use_dev_set": true,
    "use_hyperparameter_tuning": true,
    "handle_class_imbalance": true,
    "ngram_range": [
      1,
      2
    ],
    "execution_time": 279.2819721698761
  }
}